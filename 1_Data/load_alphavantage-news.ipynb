{
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat_minor": 5,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 3\n%idle_timeout 60  \n\n%%configure\n{\n  \"--datalake-formats\": \"iceberg\",\n  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n}\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.8 \nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 3\nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nThe following configurations have been updated: {'--datalake-formats': 'iceberg', '--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'}\n",
					"output_type": "stream"
				}
			],
			"id": "e7d16705-6c8c-4396-b881-9ed780813d50"
		},
		{
			"cell_type": "code",
			"source": "import boto3\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.functions import *\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 3\nIdle Timeout: 60\nSession ID: c1ff96f7-6d0c-4da7-b054-6d2e13830abb\nApplying the following default arguments:\n--glue_kernel_version 1.0.8\n--enable-glue-datacatalog true\n--datalake-formats iceberg\n--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\nWaiting for session c1ff96f7-6d0c-4da7-b054-6d2e13830abb to get into ready status...\nSession c1ff96f7-6d0c-4da7-b054-6d2e13830abb has been created.\n\n",
					"output_type": "stream"
				}
			],
			"id": "fa3e70dc-460a-449e-8ede-807520ec30be"
		},
		{
			"cell_type": "code",
			"source": "BUCKET_NAME = \"algotrading-datalake\"\nBUCKET_PREFIX = \"\"\nICEBERG_CATALOG_NAME = \"glue_catalog\"\nICEBERG_DATABASE_NAME = \"algo_data\"\nICEBERG_TABLE_NAME = \"hist_news_daily_alphavantage\"\nWAREHOUSE_PATH = f\"s3://{BUCKET_NAME}/{BUCKET_PREFIX}\"\nFULL_TABLE_NAME = f\"{ICEBERG_CATALOG_NAME}.{ICEBERG_DATABASE_NAME}.{ICEBERG_TABLE_NAME}\"",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "2df22924-9262-4f1e-8ce3-3cfc86da0987"
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .config(\"spark.sql.warehouse.dir\", WAREHOUSE_PATH) \\\n    .config(f\"spark.sql.catalog.{ICEBERG_CATALOG_NAME}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(f\"spark.sql.catalog.{ICEBERG_CATALOG_NAME}.warehouse\", WAREHOUSE_PATH) \\\n    .config(f\"spark.sql.catalog.{ICEBERG_CATALOG_NAME}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(f\"spark.sql.catalog.{ICEBERG_CATALOG_NAME}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .getOrCreate()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "736d0e2d-f729-4b56-a13e-168fab4eebad"
		},
		{
			"cell_type": "code",
			"source": "# sc = SparkContext.getOrCreate()\n\n# glueContext = GlueContext(sc)\n# spark = glueContext.spark_session\n# job = Job(spark)\n# 4. Define schema for Spark DataFrame (Good practice for Iceberg)\n# This helps ensure correct data types in Iceberg, especially for `date`.\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType\nschema = StructType([\n    StructField(\"symbol\", StringType(), True),\n    StructField(\"time_published_datetime\", DateType(), True),\n    StructField(\"sentiment_score\", DoubleType(), True)\n])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "8af11364-76c1-4de7-8538-07740c626e92"
		},
		{
			"cell_type": "code",
			"source": "import sys\nimport requests\nfrom datetime import datetime, timedelta, timezone\nimport pandas as pd\n\n\n# --- Configuration ---\nAPI_KEY = 'Z3TBUBW7GS7WSE2W' # Replace with your actual Alpha Vantage API Key\nSYMBOLS = ['INTC', 'AMD', 'NVDA']\nDAYS_BACK = 5\n\n# --- Data Fetching and Processing Classes (from your original code) ---\nclass NewsSentimentFetcher:\n    def __init__(self, api_key, days_back):\n        self.api_key = api_key\n        self.days_back = days_back\n\n    def get_time_range_str(self):\n        now = datetime.now(timezone.utc)\n        time_to = now.strftime('%Y%m%dT%H%M')\n        time_from = (now - timedelta(days=self.days_back)).strftime('%Y%m%dT%H%M')\n        return time_from, time_to\n\n    def fetch_news_for_symbol(self, symbol):\n        time_from, time_to = self.get_time_range_str()\n\n        url = (\n            f'https://www.alphavantage.co/query?function=NEWS_SENTIMENT'\n            f'&tickers={symbol}&apikey={self.api_key}'\n            f'&time_from={time_from}&time_to={time_to}&limit=1000'\n        )\n \n        response = requests.get(url)\n        if response.status_code == 200:\n            data = response.json()\n            return data.get('feed', [])\n        else:\n            print(f\"Failed to fetch news for {symbol}\")\n            return []\n\nclass NewsRecord:\n    def __init__(self, symbol, article):\n        self.symbol = symbol\n        self.time_published = article.get('time_published') # Original string\n        self.sentiment_score = self.extract_sentiment_score(article, symbol)\n\n        # Convert time_published string to datetime object\n        self.published_datetime = self._parse_time_published_to_datetime(self.time_published)\n        \n        # Extract date string (YYYY-MM-DD) from the datetime object\n        self.date = self._extract_date_str(self.published_datetime)\n\n    @staticmethod\n    def _parse_time_published_to_datetime(time_published_str):\n        \"\"\"Parses a YYYYMMDDTHHMMSS string into a datetime object.\"\"\"\n        if time_published_str:\n            try:\n                # Alpha Vantage time_published format is YYYYMMDDTHHMMSS\n                return datetime.strptime(time_published_str, '%Y%m%dT%H%M%S')\n            except ValueError:\n                # Handle cases where seconds might be missing or format is slightly different\n                try:\n                    return datetime.strptime(time_published_str, '%Y%m%dT%H%M')\n                except Exception:\n                    return None\n        return None\n\n    @staticmethod\n    def _extract_date_str(dt_obj):\n        \"\"\"Extracts date in YYYY-MM-DD format from a datetime object.\"\"\"\n        if dt_obj:\n            return dt_obj.strftime('%Y-%m-%d')\n        return None\n\n    @staticmethod\n    def extract_sentiment_score(article, symbol):\n        for ts in article.get('ticker_sentiment', []):\n            if ts.get('ticker') == symbol:\n                try:\n                    return float(ts.get('ticker_sentiment_score'))\n                except (TypeError, ValueError):\n                    return None\n        return None\n\n    def to_dict(self):\n        return {\n            # 'date': self.date, # YYYY-MM-DD string - Removed as requested\n            'symbol': self.symbol,\n            # 'time_published_str': self.time_published, # Original string - Removed as requested\n            'time_published_datetime': self.published_datetime, # Datetime object\n            'sentiment_score': self.sentiment_score,\n        }\n\n\nfetcher = NewsSentimentFetcher(API_KEY, DAYS_BACK)\nnews_records = []",
			"metadata": {
				"trusted": true,
				"tags": [],
				"scrolled": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "6c75a5a8"
		},
		{
			"cell_type": "code",
			"source": "# 2. Fetch data\nfor symbol in SYMBOLS:\n    news = fetcher.fetch_news_for_symbol(symbol)\n    for article in news:\n        record = NewsRecord(symbol=symbol, article=article)\n        if record.sentiment_score is not None:\n            news_records.append(record.to_dict())",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=INTC&apikey=Z3TBUBW7GS7WSE2W&time_from=20241202T0100&time_to=20241231T2359&limit=1000\nhttps://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=AMD&apikey=Z3TBUBW7GS7WSE2W&time_from=20241202T0100&time_to=20241231T2359&limit=1000\nhttps://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers=NVDA&apikey=Z3TBUBW7GS7WSE2W&time_from=20241202T0100&time_to=20241231T2359&limit=1000\n",
					"output_type": "stream"
				}
			],
			"id": "6cadc908-4a32-4b6b-b450-c0cc9be81325"
		},
		{
			"cell_type": "code",
			"source": "# 3. Create list\nfor article in news:\n    record = NewsRecord(symbol=symbol, article=article)\n    if record.sentiment_score is not None:\n        news_records.append(record.to_dict())\n\n# 4. Create Pandas DataFrame\npandas_df = pd.DataFrame(news_records)\n\n# 5. Convert Pandas DataFrame to Spark DataFrame\nspark_df = spark.createDataFrame(pandas_df, schema=schema)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
					"output_type": "stream"
				}
			],
			"id": "2fd88c10-be84-4309-ae5c-b5fde35a0ef0"
		},
		{
			"cell_type": "code",
			"source": "try:\n    # Try to append first\n    spark_df.writeTo(FULL_TABLE_NAME).append()\nexcept Exception as append_err:\n    try:\n        (\n            spark_df.writeTo(FULL_TABLE_NAME)\n            .using(\"iceberg\")\n            .partitionedBy(\"time_published_datetime\")  # Partitioning\n            .tableProperty(\"format-version\", \"2\")  # Optional Iceberg version\n            .create()\n        )\n    except Exception as create_err:\n        print(f\"Failed to create Iceberg table: {create_err}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": [],
			"id": "be131f7a-cea0-4c06-a016-48d662133eef"
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "cf11aa37-d08b-44d4-b31d-03d78560333f"
		}
	]
}