{
	"metadata": {
		"kernelspec": {
			"display_name": "Glue PySpark",
			"language": "python",
			"name": "glue_pyspark"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "Python_Glue_Session",
			"pygments_lexer": "python3"
		}
	},
	"nbformat_minor": 5,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "code",
			"source": "%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 3\n%idle_timeout 60  \n\n%%configure\n{\n  \"--datalake-formats\": \"iceberg\",\n  \"--conf\": \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n}\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.8 \nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 3\nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 60 minutes.\nThe following configurations have been updated: {'--datalake-formats': 'iceberg', '--conf': 'spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'}\n",
					"output_type": "stream"
				}
			],
			"id": "2173bfd3-d0b0-4f25-beca-5a4faf553f3d"
		},
		{
			"cell_type": "code",
			"source": "import boto3\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom pyspark.sql.functions import *\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 3\nIdle Timeout: 60\nSession ID: 8e4bd850-7291-4ec9-a11c-669ab663c9c6\nApplying the following default arguments:\n--glue_kernel_version 1.0.8\n--enable-glue-datacatalog true\n--datalake-formats iceberg\n--conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\nWaiting for session 8e4bd850-7291-4ec9-a11c-669ab663c9c6 to get into ready status...\nSession 8e4bd850-7291-4ec9-a11c-669ab663c9c6 has been created.\n\n",
					"output_type": "stream"
				}
			],
			"id": "fa3e70dc-460a-449e-8ede-807520ec30be"
		},
		{
			"cell_type": "code",
			"source": "BUCKET_NAME = \"algotrading-datalake\"\nBUCKET_PREFIX = \"\"\nICEBERG_CATALOG_NAME = \"glue_catalog\"\nICEBERG_DATABASE_NAME = \"algo_data\"\nICEBERG_TABLE_NAME = \"hist_ohlcv_daily_alphavantage\"\nWAREHOUSE_PATH = f\"s3://{BUCKET_NAME}/{BUCKET_PREFIX}\"\nFULL_TABLE_NAME = f\"{ICEBERG_CATALOG_NAME}.{ICEBERG_DATABASE_NAME}.{ICEBERG_TABLE_NAME}\"",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "2df22924-9262-4f1e-8ce3-3cfc86da0987"
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n    .config(\"spark.sql.warehouse.dir\", WAREHOUSE_PATH) \\\n    .config(f\"spark.sql.catalog.{ICEBERG_CATALOG_NAME}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n    .config(f\"spark.sql.catalog.{ICEBERG_CATALOG_NAME}.warehouse\", WAREHOUSE_PATH) \\\n    .config(f\"spark.sql.catalog.{ICEBERG_CATALOG_NAME}.catalog-impl\", \"org.apache.iceberg.aws.glue.GlueCatalog\") \\\n    .config(f\"spark.sql.catalog.{ICEBERG_CATALOG_NAME}.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n    .getOrCreate()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "736d0e2d-f729-4b56-a13e-168fab4eebad"
		},
		{
			"cell_type": "code",
			"source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType\nschema = StructType([\n    StructField(\"dt\", DateType(), True),   \n    StructField(\"symbol\", StringType(), True),\n    StructField(\"open\", DoubleType(), True),\n    StructField(\"high\", DoubleType(), True),\n    StructField(\"low\", DoubleType(), True),\n    StructField(\"close\", DoubleType(), True),\n    StructField(\"volume\", DoubleType(), True)        \n])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "8af11364-76c1-4de7-8538-07740c626e92"
		},
		{
			"cell_type": "code",
			"source": "import sys\nimport requests\nfrom datetime import datetime, timedelta, timezone\nimport pandas as pd\n\n\n# --- Configuration ---\nAPI_KEY = '27MOH0ZJ8C2TTVW6' # Replace with your actual Alpha Vantage API Key\nSYMBOLS = ['INTC']\nDAYS_BACK = 5\n\n# --- Data Fetching and Processing Classes (from your original code) ---\nclass OHLCVFetcher:\n    def __init__(self, api_key, days_back):\n        self.api_key = api_key\n        self.days_back = days_back\n\n    def get_time_range_str(self):\n        now = datetime.now(timezone.utc)\n        time_to = now.strftime('%Y%m%dT%H%M')\n        time_from = (now - timedelta(days=self.days_back)).strftime('%Y%m%dT%H%M')\n        return time_from, time_to\n\n    def fetch_ohlcv_for_symbol(self, symbol):\n        time_from, time_to = self.get_time_range_str()\n        url = (\n            f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY'\n            f'&symbol={symbol}&apikey={self.api_key}'\n            f'&outputsize=full'\n\n        )\n        response = requests.get(url)\n        if response.status_code == 200:\n            data = response.json()\n            return data.get('Time Series (Daily)', [])\n        else:\n            print(f\"Failed to fetch OHLCV for {symbol}\")\n            return []\n\nclass OHLCVRecord:\n    def __init__(self, date, values, symbol):\n        self.symbol = symbol\n        self.open = values.get('1. open') # Original string\n        self.high = values.get('2. high') # Original string\n        self.low = values.get('3. low') # Original string\n        self.close = values.get('4. close') # Original string\n        self.volume = values.get('5. volume') # Original string        \n        # Extract date string (YYYY-MM-DD) from the datetime object\n        self.date = date\n\n    def to_dict(self):\n        return {\n            'dt': self.date, # YYYY-MM-DD string - Removed as requested\n            'symbol': self.symbol,\n            'open': float(self.open),\n            'high': float(self.high), # Datetime object\n            'low': float(self.low), # Datetime object\n            'close': float(self.close),\n            'volume': float(self.volume),            \n        }\n\n\nfetcher = OHLCVFetcher(API_KEY, DAYS_BACK)\nohlcv_records = []",
			"metadata": {
				"trusted": true,
				"tags": [],
				"scrolled": true
			},
			"execution_count": 21,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "6c75a5a8"
		},
		{
			"cell_type": "code",
			"source": "for symbol in SYMBOLS:\n    ohlcv = fetcher.fetch_ohlcv_for_symbol(symbol)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "6cadc908-4a32-4b6b-b450-c0cc9be81325"
		},
		{
			"cell_type": "code",
			"source": "for date, values in ohlcv.items():\n    ohlcv_tick = OHLCVRecord(date=date, values = values, symbol = symbol)\n    ohlcv_records.append(ohlcv_tick.to_dict())",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 22,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			],
			"id": "25a003c6-12fe-414a-915b-204170234ed8"
		},
		{
			"cell_type": "code",
			"source": "# 4. Create Pandas DataFrame\npandas_df = pd.DataFrame(ohlcv_records)\npandas_df['dt'] = pd.to_datetime(pandas_df['dt']).dt.date\n\n# 5. Convert Pandas DataFrame to Spark DataFrame\nspark_df = spark.createDataFrame(pandas_df, schema=schema)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 23,
			"outputs": [
				{
					"name": "stdout",
					"text": "/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for column, series in pdf.iteritems():\n",
					"output_type": "stream"
				}
			],
			"id": "2fd88c10-be84-4309-ae5c-b5fde35a0ef0"
		},
		{
			"cell_type": "code",
			"source": "try:\n    # Try to append first\n    spark_df.writeTo(FULL_TABLE_NAME).append()\nexcept Exception as append_err:\n    try:\n        (\n            spark_df.writeTo(FULL_TABLE_NAME)\n            .using(\"iceberg\")\n            .partitionedBy(\"dt\")  # Partitioning\n            .tableProperty(\"format-version\", \"2\")  # Optional Iceberg version\n            .create()\n        )\n    except Exception as create_err:\n        print(f\"Failed to create Iceberg table: {create_err}\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": [],
			"id": "be131f7a-cea0-4c06-a016-48d662133eef"
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": [],
			"id": "013907d9-a010-4e58-8a2b-59df78e8dfc9"
		}
	]
}